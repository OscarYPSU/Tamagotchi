Current implementation consists of:

Face tracking and face locking using 2 MG servos for rotational and a esp32-s3 with OV5680 camera module to capture live video which is scraped in a python backend for facial reconition processing nd coordination analyst to send servo coordinates over to microcontroller so microcontroller knows where to move in terms of x,y axis

current implementation also has emotional recognition 

form of communication between python backend server and microntroler is a UART form utilizing serial and custom prcessed data.  

thge scraping of live streaming from esp32-s3 is done so by usiung CV2 library on a custom IP that only shows the live streaming . aka a :81 port

utilizes multi threading to handle continuas scpraing of live streaming and for facial recognition and sending UART data on the side 

emotional recognition is done so by using a FER library

Moved CameraWebServer files to a new directory. Refactored OLED display code to use a shared display object and modularized emoticon rendering (happy, angry, study faces) with new header and source files. Added multi-threaded emotion data sending in FacialRecognition.py and updated rotational sketch to support emotion-based display switching. Added a project journal for documentation.


Problem:
Your OLED display initialization (display.begin(...)) kept failing, printing “SSD1306 allocation failed” and entering the infinite loop. However, when you removed the studySetup() call, it suddenly worked fine.
Cause:
studySetup() was being called immediately after display.begin(), before the display was fully ready or before other hardware (like I²C devices or pins) finished initializing. Inside studySetup(), something was interfering with the I²C communication — possibly reinitializing the Wire bus, using the same pins, or drawing to the display before it was ready. That caused display.begin() to fail because the OLED never received the proper I²C handshake.
Solution:
You added a short delay after display.begin() (for example, delay(100);) to give the OLED time to stabilize before calling studySetup(). That allowed the display initialization to complete properly, so the program could proceed without hitting the allocation error.



Introduced pet stats (hunger, health, sleepiness, happiness, age) and their update intervals in CentreFace.ino. Updated study.cpp to use new animation frames and removed the old book cartoon bitmap. Modified FacialRecongnition.py to send emotions in lowercase. Added empty DB/insert.py and DB/retrieve.py for future database operations.


Problem/Conflict:
While developing a simple game on an Arduino with an OLED display, I wanted to animate a character using multiple bitmap frames. My first instinct was to use delay() for timing the frames. However, I quickly realized that using delay() blocked the rest of the code, meaning I couldn’t update the game state or read input while the animation was running. I also considered multi-threading to handle animations separately, but the Arduino Uno doesn’t support threads, so that approach wasn’t possible.
I needed a solution that would allow the animation to run smoothly while still allowing other tasks like checking player input, updating scores, and handling collisions.
Solution:
I implemented a non-blocking timer approach using millis(). I kept track of the last frame update and compared it with the current time. Whenever enough time had passed, I updated the animation frame and reset the timer. This allowed the loop to continue running other tasks in parallel with the animation, essentially simulating multitasking without real threads.
Result/Outcome:
The character animation ran smoothly at the desired frame rate, and the game could still respond to input, update the state, and handle other logic simultaneously. It taught me a valuable lesson about working within the constraints of microcontrollers and designing code to be efficient and non-blocking, even without advanced features like threading.

Problem:
While working on a microcontroller project, I needed to share a timing variable (unsigned long millis) across multiple .cpp files. Initially, I defined the variable directly in the header file. This caused linker errors because each file that included the header created its own separate definition, violating the One Definition Rule in C++.
Solution:
I learned that global variables should only be declared in headers and defined in a single .cpp file. To fix the issue, I used the extern keyword in the header to declare the variable and initialized it only once in the main .cpp file. This ensured all modules referenced the same variable, eliminating 


Introduced neutralFace.cpp and neutralFace.h for a new emoticon. Added milis.h for millis tracking. Removed unnecessary display includes from headers, deleted display.cpp, and updated CentreFace.ino to set up the neutral face on startup. duplicate symbol errors.

Consolidated multiple face emotion implementations into a single 'emotions' module, removing redundant face files and headers. Added servo control abstraction and refactored the main Arduino sketch to use new emotion and servo modules. Introduced a Flask-based web interface with a simple pet logic backend, including real-time hunger display, and added serial communication scripts. Cleaned up unused code and improved modularity for easier maintenance and extension. (wrote HTTPS endpoints for getting data) Used asyncronsouse functrion in javacript in html for continuously update in hunger display using fetch

Refactored pet logic into a Pet class with thread-safe stat updates and new API endpoints for hunger and happiness. Updated Serial.py to add emotion detection and face tracking, sending data to a microcontroller. Improved the web interface to support feeding and display happiness, and updated Flask routes for RESTful API design.

Introduced new API endpoints and logic for sleepiness and play actions in the virtual pet application. Updated the Pet class to handle sleep and play states, including periodic stat updates based on pet status. Enhanced the frontend to display and interact with sleepiness and happiness, allowing users to play with or put the pet to sleep via new buttons and asynchronous JavaScript functions.

Moved and renamed logic.py to pet.py, and updated Main.py to use the new PetLogic modules. Added petFood.py with a basic Food class and Cupcake subclass. Moved static and template files to new locations, splitting JavaScript into static/home.js and updating home.html to reference it. Added redisTest.py for Redis and PostgreSQL integration testing. Minor improvements to Serial.py and pet logic, including a ToggleSleep method.

Developed a secure web application using Flask, Flask-Login, and Redis for session management.

Implemented user authentication and registration with PostgreSQL as the backend database, including INSERT and SELECT queries for storing and verifying credentials.

Integrated Redis to manage user sessions in a scalable and efficient manner, enabling session persistence across server restarts.

Designed a login workflow with session-based access control, protecting routes using @login_required decorators.

Built dynamic templates using render_template_string to handle login and dashboard views.

Applied secure coding practices by using parameterized queries (%s) to prevent SQL injection attacks.

Managed database connections and cursors responsibly to ensure data integrity.

Technologies used: Python, Flask, Flask-Login, Flask-Session, Redis, PostgreSQL (psycopg2), HTML forms.

Developed a real-time digital pet simulation using Flask and Flask-Login for user session management.

Designed RESTful API endpoints (GET, POST, PUT) to interact with the pet’s state, including hunger, happiness, and sleepiness, enabling dynamic updates from the frontend.

Implemented backend game logic in a separate thread to continuously update pet attributes independently of web requests.

Used JSON responses to provide a clean interface between backend logic and frontend templates.

Created interactive routes for feeding, playing, and sleeping, demonstrating event-driven state changes.

Integrated Python classes and object-oriented design (Pet and PetFood) to manage game mechanics modularly.

Technologies used: Python, Flask, Flask-Login, threading, RESTful APIs, JSON, HTML templates.

Developed a real-time face tracking and emotion detection system using OpenCV and FER (Facial Emotion Recognition).

Captured live video streams from an ESP32 camera and processed frames to detect faces and extract top emotions.

Designed multi-threaded Python architecture to simultaneously:

Track face positions and send coordinates to a microcontroller via serial communication.

Detect emotions and transmit emotion data to an OLED display connected to the microcontroller.

Applied Haar Cascade classifiers for fast face detection and optimized frame processing for real-time performance.

Implemented data encoding and transmission over serial ports to interface with embedded devices, enabling responsive interaction between software and hardware.

Added debugging and visualization features using OpenCV to display detected faces, emotion labels, and tracking coordinates.

Technologies used: Python, OpenCV, FER, threading, serial communication, ESP32 camera, real-time data streaming.


started playing around with flask sessoina dn redis db to store session for login caches. Connected Redis DB using flask session as the database to store authentication session keys. Created queries and functions for queries in postgresql db. used a server side session storage for authetnicaiton. Used parameterized queries and created a custom hashed password encryption before storing passwrods

used a multithread connection pool safe to maange connections to database to prevent dataleak. Used contextmanager to prevent forgetful of closing connection(does it auotmicatlly)

Added PostgreSQL connection pooling and query functions for user authentication in DB/mainQuery.py. Updated CookBook/redisTest.py and Main.py to use Redis for session management and PostgreSQL for user queries. Improved login logic and session handling, and added a basic login HTML template. Minor bug fix in PetLogic/pet.py for sleepiness update logic.

properly encrypted user password before putting iont o database and for checking authentication


Added PostgreSQL connection pooling and bcrypt password hashing for secure user authentication in DB/mainQuery.py. Integrated Redis for session management and updated Main.py to handle login, registration, and logout routes. Improved home and login templates to support authentication flow and display user information. Added register.html for user registration.
problem:During encrypting and decrypting process, i kept gettign error due to mismatch data types for example the encoded password returned bytes which was too long to be sotred in current database architucture and i also had to decode it to a stringso i ahd to update that then whenerv i retrieved the password from the database, it would come as a string and i didnt realize i needed to encode it back to a byte to check

Introduced a pet sprite display on the home page with new CSS styling and image asset. Enhanced Main.py with improved comments for user session management and authentication routes.
Added sprite image aniamtion that runs in html itself

added a separate table caleld user data to store user pet data and utilized foregin key constraint to ensure data integrity between the two tables
Moved authentication logic to a new DB/authenticationQuery.py module and user data logic to DB/userDataQuery.py. Introduced a connection pool in DB/connectionPool.py for database access. Updated Main.py to use the new authentication and user data modules, and refactored pet logic to initialize pet state from user data. Simplified and modularized database queries for better maintainability and scalability.
added persistent data for user pet data in a separte table in postgresql data 

Introduces a global thread and pet instance management system to handle pet logic for all users, starting threads for each user at app startup. Updates pet logic to synchronize pet stats with the database on each loop iteration, and fixes hunger and happiness update logic. Refactors Flask routes to use the correct user pet instance from the session.

Updated pet status initialization and stat boundaries in Pet class. Refactored facial recognition to run in a separate thread for smoother operation. Fixed typo in Main.py and improved web start/logout flow. Moved static assets to the static directory and updated home.html to reference the new static paths.

Introduces a new GraphQL API in CookBook/graphqlapi.py and a placeholder schemagraph.py. Updates Main.py to flush Redis sessions on startup and set session lifetime. Fixes variable usage in Serial.py for emotion detection and frame center drawing. Updates .gitignore and adds requirements.txt for dependency management.
Started using a venv for project, working w linux etc

Reorganized BLETest into PlatformIO format under BLETest/BLETest, added .gitignore, VSCode config, and documentation files. Migrated and updated platformio.ini, replaced BLETest.ino with src/main.cpp implementing BLE LED control via BLE characteristic.

Starting using a venv to ensure that it odesnt only work on my laptop

Introduces MyServerCallbacks in main.cpp to handle BLE device connect/disconnect events and restart advertising on disconnect. Adds Scripts/BLETEST.PY for scanning and connecting to BLE devices using Bleak. Updates Journal.txt with venv usage note.
had problem with MCU not being able to be connected again after being disconnected due to hanging connection from script ending.

Added esp2.cpp implementing a BLE server for a second ESP32-S3 MCU with custom connect/disconnect callbacks and advertising restart. Updated platformio.ini to define two MCU environments with selective source inclusion. Enhanced BLETEST.PY to support repeated user input for BLE characteristic writes and improved disconnect handling. Updated Journal.txt with notes on venv usage and BLE reconnect issue.
using python script to start both MCU and run extensive testing on them
Having trouble understand between sending BYTES and standard string. Need to learn how machine interact w bytes and normal values like int and strings

Added BLETEST-Client.py for BLE communication via Python. Refactored PlatformIO config to use client.cpp and server.cpp for two MCU environments, replacing esp2.cpp and main.cpp. Renamed and relocated BLETEST-Server.PY. Updated Journal.txt with notes on byte/string handling. Added notes and terminal script documentation for development workflow. 
-Added a client ESP32-S3, advertising for python backend to connect to for remote control then starts scanning for other MCU and connecting to the appriopiate using serviceUUID (thinking of implementing custom handshake protocol 

Reorganized global variables and added functions for sending data to the server. Enhanced the callback to flag new data and trigger sending messages to a connected device. Minor prompt update in the Python client script.
basically one MCU advertise and my pthon connected via BLE then that one MCU is connected to another MCU my pthon sends data over to that one MCU whichin returns that one MCU said data to the second MCU to turn of LED